{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78d61156",
   "metadata": {},
   "source": [
    "# 00 — Project Overview: Chaos vs Stochasticity Classification\n",
    "\n",
    "**Author:** Hafeez  | **University of Cambridge**, Department of Applied Mathematics and Theoretical Physics | February 2026\n",
    "\n",
    "---\n",
    "\n",
    "This notebook lays out my thinking going into this project. It is meant to be read before anything else in the repository.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f2415f",
   "metadata": {},
   "source": [
    "## 1. The Problem\n",
    "\n",
    "Given a raw time series, can we tell whether it came from a **deterministic chaotic system** or a **stochastic process**?\n",
    "\n",
    "Both can look random to the eye. But the distinction matters enormously:\n",
    "\n",
    "- If the process is **deterministic**, there is hidden structure we can exploit for potential short-term forecasting\n",
    "- If the process is **stochastic**, the randomness is fundamental and we are limited to statistical descriptions.\n",
    "\n",
    "The two mechanisms produce time series that can be very difficult to tell apart visually. The goal of this project is to train a neural network that learns to make this distinction automatically from raw data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d254c43",
   "metadata": {},
   "source": [
    "## 2. Why It Matters\n",
    "\n",
    "**Quantitative Finance**\n",
    "\n",
    "Markets alternate between regimes. Sometimes prices exhibit structured, exploitable patterns (deterministic-like); other times behaviour is dominated by noise. A chaos classifier can act as a regime detector — signalling when systematic strategies are likely to work and when to pull back. It directly informs position sizing, entry/exit timing, and risk management.\n",
    "\n",
    "**Climate Science**\n",
    "\n",
    "Climate data contains both deterministic components (ENSO, ocean circulation) and stochastic components (weather noise, volcanic forcing). Distinguishing the two is essential for model validation and for understanding how far into the future different phenomena can be predicted.\n",
    "\n",
    "**Biomedical Signals**\n",
    "\n",
    "EEG and ECG signals mix deterministic dynamics with noise. Detecting transitions between regimes (e.g., onset of an epileptic seizure) has direct clinical applications.\n",
    "\n",
    "**Seismic Activity**\n",
    "\n",
    "Tracking the slippping of faults\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da822ec3",
   "metadata": {},
   "source": [
    "## 3. The Key Idea: Permutation Entropy\n",
    "\n",
    "The approach is rooted in the **Omega metric** introduced by Boaretto et al. (2021).\n",
    "\n",
    "The core mathematical tool is **permutation entropy** (Bandt & Pompe, 2002). The idea is simple:\n",
    "\n",
    "1. Take a time series and look at short consecutive subsequences of length $m$ they use 6 in the paper, my theory is because a subsequence of length six gives enough information for a potential correlation but it's not too long so that you lose a lot of the variability.\n",
    "2. For each subsequence, record only the **rank ordering**, essentially see orders from smallest to largest and the volatile nature of the ranking should allude to some type of structure... or lack thereof. \n",
    "3. Count how often each possible pattern appears\n",
    "4. Compute the Shannon entropy of this distribution\n",
    "\n",
    "\n",
    "The CNN we build will learn to extract equivalent information directly from the raw data with no manual feature engineering required.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123bc690",
   "metadata": {},
   "source": [
    "## 4. Data Generation Plan\n",
    "\n",
    "We generate all training data synthetically so that we have **perfect ground-truth labels**. Data generation is done in **Julia** for two reasons:\n",
    "\n",
    "1. `DynamicalSystems.jl` provides high-accuracy ODE integrators purpose-built for chaotic systems (adaptive step sizes, tight error tolerances). This matters because chaotic systems amplify numerical errors exponentially.\n",
    "2. Julia is significantly faster than Python for the tight numerical loops in discrete maps and delay equations.\n",
    "\n",
    "### Deterministic Systems (Script `01_deterministic_systems.jl`)\n",
    "\n",
    "| System | Type | \n",
    "|--------|------|\n",
    "| Lorenz attractor | 3D continuous ODE\n",
    "| Rössler attractor | 3D continuous ODE\n",
    "| Logistic map | 1D discrete map \n",
    "| Hénon map | 2D discrete map \n",
    "| Mackey-Glass | Delay DE \n",
    "\n",
    "Each system is generated with multiple parameter configurations and all three components (where applicable) to maximise diversity.\n",
    "\n",
    "### Stochastic Processes (Script `02_stochastic_processes.jl`)\n",
    "\n",
    "| Process | Why include it? |\n",
    "|---------|-----------------|\n",
    "| White noise | Baseline — no temporal correlation at all |\n",
    "| Coloured noise (1/f^β) | Introduces power-law correlations that could fool a naive classifier |\n",
    "| Random walk | Non-stationary; tests robustness |\n",
    "| ARMA(p,q) | Linear temporal dependence — the \"hard\" stochastic case |\n",
    "| Ornstein-Uhlenbeck | Mean-reverting; widely used in finance |\n",
    "\n",
    "Each process is generated with multiple parameter values and five independent realisations per configuration.\n",
    "\n",
    "### Augmentation\n",
    "\n",
    "Every deterministic series is also duplicated with **additive Gaussian noise** at SNR levels of 30, 25, 20, 15, and 10 dB. This forces the CNN to detect deterministic structure even when partially obscured.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55c44fb",
   "metadata": {},
   "source": [
    "## 5. CNN Architecture Plan\n",
    "\n",
    "We use a **1D Convolutional Neural Network** because:\n",
    "- 1D convolutions naturally detect **temporal patterns** in sequential data\n",
    "- They provide **translation invariance** — the same pattern is detected wherever it appears in the window\n",
    "- They are computationally efficient compared to recurrent architectures\n",
    "- They learn their own feature extractors end-to-end from raw data\n",
    "\n",
    "### The Design\n",
    "\n",
    "```\n",
    "Input: raw time series window (512 points)\n",
    "\n",
    "→ Multi-Scale Conv Block\n",
    "    Three parallel 1D convolutions (kernel sizes 3, 7, 15)\n",
    "    Captures local, medium, and long-range temporal patterns\n",
    "    Output: 64 channels\n",
    "\n",
    "→ Residual Block 1 (64 → 64, kernel size 7) → MaxPool\n",
    "→ Residual Block 2 (64 → 128, kernel size 5) → MaxPool\n",
    "→ Residual Block 3 (128 → 256, kernel size 3)\n",
    "\n",
    "→ Global Average Pooling\n",
    "→ Fully Connected: 256 → 128 → 1\n",
    "→ Sigmoid → P(deterministic)\n",
    "```\n",
    "\n",
    "**Multi-scale convolutions** let the first layer capture patterns at three temporal resolutions simultaneously — analogous to computing permutation entropy at different embedding dimensions.\n",
    "\n",
    "**Residual connections** add the input of each block directly to its output, which prevents gradient degradation and lets the network learn identity mappings where adding complexity would not help.\n",
    "\n",
    "**Global average pooling** collapses the temporal dimension by averaging, giving a fixed-size representation regardless of where patterns appear.\n",
    "\n",
    "### Training Strategy\n",
    "\n",
    "| Setting | Value | Why |\n",
    "|---------|-------|-----|\n",
    "| Loss | Binary cross-entropy with logits | Numerically stable for binary classification |\n",
    "| Optimiser | AdamW | Decoupled weight decay; solid default |\n",
    "| Learning rate | 1e-3, halved on plateau | Standard starting point with adaptive reduction |\n",
    "| Early stopping | Patience 15 epochs | Prevents overfitting; restores best weights |\n",
    "| Dropout | 0.2 | Regularisation throughout the network |\n",
    "| Gradient clipping | Max norm 1.0 | Prevents exploding gradients |\n",
    "| Batch size | 64 | Balance between gradient noise and speed |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f336d9a8",
   "metadata": {},
   "source": [
    "## 7. Project Roadmap\n",
    "\n",
    "| # | Notebook / Script | Language | What it does |\n",
    "|---|-------------------|----------|--------------|\n",
    "| **00** | `00_project_overview.ipynb` (this) | Python | Motivation, plan, and preliminary demonstration |\n",
    "| **01** | `01_deterministic_systems.jl` | Julia | Generate chaotic time series (Lorenz, Rössler, logistic, Hénon, Mackey-Glass) |\n",
    "| **02** | `02_stochastic_processes.jl` | Julia | Generate stochastic time series (white/coloured noise, random walk, ARMA, OU) |\n",
    "| **03** | `03_dataset_assembly.ipynb` | Python | Load the Julia CSVs, window, add noise, split into train/val/test |\n",
    "| **04** | `04_cnn_architecture.ipynb` | Python | Define the CNN layer by layer with full explanations |\n",
    "| **05** | `05_training_pipeline.ipynb` | Python | Train with AdamW, LR scheduling, early stopping |\n",
    "| **06** | `06_evaluation.ipynb` | Python | Test set metrics, confusion matrix, ROC curve, confidence analysis |\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "1. Boaretto, B.R.R. et al. (2021). *Evaluating temporal correlations in time series using permutation entropy, ordinal probabilities and machine learning.* Chaos, 31(6), 063124.\n",
    "2. Bandt, C. & Pompe, B. (2002). *Permutation entropy: a natural complexity measure for time series.* Physical Review Letters, 88(17), 174102.\n",
    "3. Lorenz, E.N. (1963). *Deterministic nonperiodic flow.* Journal of the Atmospheric Sciences, 20(2), 130–141.\n",
    "\n",
    "---\n",
    "\n",
    "*Proceed to `01_deterministic_systems.jl` to begin generating the training data.*\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
